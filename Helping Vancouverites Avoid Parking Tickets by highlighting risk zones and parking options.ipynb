{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helping Vancouverites avoid Parking Tickets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More than 3.7 million parking tickets have been issued in Vancouver over the past decade with drivers shelling out an average of <font color=red>$ 32 million dollars</font>  every year.\n",
    "\n",
    "The average fine of a single ticket is around <font color=blue>$82 dollars</font>. It is a fact that residents need better parking-know how. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project aims to combine both parking tickets dataset and parking meters dataset to make customized recommendations  to park to drivers based on different circumstances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### I will start by importing all the requisite libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import time \n",
    "import folium\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import glob\n",
    "from PIL import Image\n",
    "\n",
    "import geopandas as gpd\n",
    "\n",
    "from shapely.geometry import Point\n",
    "import matplotlib.colors as colors\n",
    "\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "\n",
    "%matplotlib inline\n",
    "from shapely.geometry import Polygon, Point, MultiPolygon\n",
    "import shapefile\n",
    "import googlemaps\n",
    "import folium\n",
    "from sklearn.metrics import silhouette_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The datasets have been collected from the Open Data Portal provided by the City of Vancouver. The website can be found here https://opendata.vancouver.ca/explore/?q=ope+dat&disjunctive.features&disjunctive.theme&disjunctive.keyword&disjunctive.data-owner&disjunctive.data-team&sort=modified.\n",
    "\n",
    "There are a total of 4 CSV files organized in order of ascending years starting from 2010 till 2020. \n",
    "\n",
    "Since the CSV files were semicolon delimited, I had to specify the appropriate parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parking_tickets_2010to2013 = pd.read_csv('VancouverParkingTickets/parking-tickets-2010-2013.csv', sep = ';')\n",
    "parking_tickets_2010to2013"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parking_tickets_2014to2017 = pd.read_csv('VancouverParkingTickets/parking-tickets-2014-2016.csv', sep = ';')\n",
    "parking_tickets_2014to2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parking_tickets_2017to2019 = pd.read_csv('VancouverParkingTickets/parking-tickets-2017-2019-3.csv', sep = ';')\n",
    "parking_tickets_2017to2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parking_tickets_2020 = pd.read_csv('VancouverParkingTickets/parking-tickets.csv', sep = ';')\n",
    "parking_tickets_2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Now, I will read in the parking meters data, which is one complete file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parking_meters = pd.read_csv('VancouverParkingTickets/parking-meters-2.csv', sep = ';')\n",
    "parking_meters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For ease of computation, I will merge the parking tickets datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [parking_tickets_2010to2013, parking_tickets_2014to2017, parking_tickets_2017to2019, parking_tickets_2020]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Total_Parking_Tickets= pd.concat(frames)\n",
    "Total_Parking_Tickets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are more than 3.7 million entries here. Let us take a look at what kind of data we are dealing with. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Total_Parking_Tickets.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BI_ID is the  key field added during import to data warehouse. It is not useful for our purposes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Total_Parking_Tickets= Total_Parking_Tickets.drop(['BI_ID'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us see if there are any duplicated rows in our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Total_Parking_Tickets.duplicated().value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "About 40% of the rows in our data are duplicate. However, we cannot drop them. Because it is entirely possible that parking enforcement officer or a group of parking enforcement officer issued tickets at the same location on the same date for the same offence. You notice here that the exact time is not given, which could have assisted us in this matter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will look at null values now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Total_Parking_Tickets.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us confirmation that the City department has provided us with clean data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, I will repeat the process for the parking meters dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parking_meters.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parking_meters.duplicated().value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no duplicated meters. What about null values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parking_meters.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will remove the Rate Misc and Time Misc columns since more than 50% of these columns are null values and also because we will not be using them in our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parking_meters = parking_meters.drop(['RATE_MISC', 'TIME_MISC'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parking_meters['METERHEAD'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us take a look at the remaining null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parking_meters.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parking_meters.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since they are very few in proportion to the total value counts we can remove the rows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parking_meters= parking_meters.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us verify that we have a clean dataset to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parking_meters.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Addressing Geo locations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, since we are dealing with geo-location data, we will ensure this data is in the correct format as per our needs. I will start by modifying the parking_meters dataset first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parking_meters['Geom'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am looking to convert the \"Geom\" column into 2 separate columns - Latitude and Longitude. Let us look at what type of data is used in this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parking_meters.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since, it is a object type column, we can remove the words \"type\", \"Point\" and \"coordinates\" from the column name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parking_meters['Geom'] = parking_meters['Geom'].str.replace(r'{\"type\": \"Point\", \"coordinates\":', '')\n",
    "                                                            \n",
    "parking_meters\n",
    "                                                            \n",
    "                  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can split the column into 2 parts as mentioned before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parking_meters[['Longitude','Latitude']] = parking_meters.Geom.str.split(\",\",expand=True)\n",
    "parking_meters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us clean the remaining sections of both these new columns and also drop the original column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parking_meters['Longitude'] = parking_meters['Longitude'].str.replace(r'{', '')\n",
    "parking_meters['Longitude'] = parking_meters['Longitude'].str.replace(r'[', '')\n",
    "parking_meters['Latitude'] = parking_meters['Latitude'].str.replace(r']', '')\n",
    "parking_meters['Latitude'] = parking_meters['Latitude'].str.replace(r'}', '')\n",
    "\n",
    "\n",
    "                                                            \n",
    "                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parking_meters= parking_meters.drop(['Geom'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parking_meters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, it is time to work on the Total_Parking_Tickets dataset and create the location columns. We will first create a complete Address column for each parking ticket by concatenating the Block and Street columns and adding the name of the city and country. This is done in accordance to the Google API requirements which we will use later.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Total_Parking_Tickets['Address'] = Total_Parking_Tickets['Block'].map(str) + \" \" +  Total_Parking_Tickets['Street'] + \" Vancouver \" + \" Canada \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, we drop the original columns now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Total_Parking_Tickets= Total_Parking_Tickets.drop(['Block', \"Street\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Total_Parking_Tickets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us start by looking at how the parking tickets are clustered in different locations. This means we want to find out which locations are the most ticketed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = Total_Parking_Tickets['Address']\n",
    "counts = s.value_counts()\n",
    "percent100 = s.value_counts(normalize=True).mul(100).round(1).astype(str) + '%'\n",
    "Most_Ticketed_Streets =pd.DataFrame({'Count': counts, 'Percentage of Total': percent100 }).head(20)\n",
    "Most_Ticketed_Streets.columns = ['Count', 'Percentage of Total']\n",
    "Most_Ticketed_Streets.index.names = ['Location']\n",
    "Most_Ticketed_Streets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us plot the top 20 addresses here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Number of Tickets')\n",
    "plt.ylabel('No. of Violations')\n",
    "plt.xlabel('Location')\n",
    "Total_Parking_Tickets['Address'].value_counts().nlargest(20).plot.bar()\n",
    "positions = (0, 1,2, 3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19)\n",
    "labels = (\"1100 Alberni St\", \"500 8th Ave W.\", \"1000 Robson St.\",\"1100 Homer St.\",\"1000 Homer St.\",\"800 Homer St.\",\"1100 Robson St.\",\"1000 Mainland St\",\"2200 4th Ave W.\",\"1000 Cordova St.\", \"1000 Alberni St.\",\"1100 Hamilton St.\", \"1900 4th Ave W.\",\"900 Homer St.\",\"1000 Hamilton St.\",\"2100 4th Ave W.\",\"1100 Mainland St\",\"800 Richards St.\",\"800 Seymour St.\",\"2000 4th Ave W.\")\n",
    "plt.xticks(positions, labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many unique addresses are there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Total_Parking_Tickets['Address'].nunique()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us see count the total values in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Total_Parking_Tickets['Address'].value_counts().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And what number of these tickets are clustered in the top 5000 locations? This number has been derived from trial and error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Total_Parking_Tickets['Address'].value_counts().head(5000).sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that the top 5000 locations accounts for almost 3711580/3490715 ≃ **94%**  of all tickets being issued. Since our project deals with offering options to drivers where they are most likely to be ticketed, we will be using only these locations moving forward. This will help us save valuable processing time and improve the accuracy of our models as well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using Google API to get the geo-locations for our addresses. Ensure you create an account here:https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=&ved=2ahUKEwjru7W0j_jrAhWFCjQIHYCKBhgQFjAAegQIAxAB&url=https%3A%2F%2Fconsole.developers.google.com%2F&usg=AOvVaw39ieEDI7pzBj4NtuzqS57M and create an API key. Use this key to access this service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googlemaps import Client as GoogleMaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = \"INSERT API_KEY HERE\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmaps = GoogleMaps(\"INSERT API_KEY HERE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Most_Ticketed_Streets['Longitude'] = \"\"\n",
    "Most_Ticketed_Streets['Latitude'] = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember to include a time delay. This is very importnat since the system might crash due to so many requests at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from time import sleep\n",
    "for x in range(len(new_data2)):\n",
    "    try:\n",
    "        time.sleep(2) \n",
    "        geocode_result = gmaps.geocode(Most_Ticketed_Streets['index_col'][x])\n",
    "        Most_Ticketed_Streets['Latitude'][x] = geocode_result[0]['geometry']['location'] ['lat']\n",
    "        Most_Ticketed_Streets['Longitude'][x] = geocode_result[0]['geometry']['location']['lng']\n",
    "    except IndexError:\n",
    "        print(\"Address was wrong...\")\n",
    "    except Exception as e:\\\n",
    "        print(\"Unexpected error occurred.\", e )\n",
    "Most_Ticketed_Streets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we save the file to a csv for further use and to ensure data is not lost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Most_Ticketed_Streets.to_csv('Top 5000 locations.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Most_Ticketed_Streets = pd.read_csv('VancouverParkingTickets/Top 5000 locations.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us look at our file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Most_Ticketed_Streets\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset does contain some null values. These are the locations Google API could not derive the coordinates for. Let us remove them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Most_Ticketed_Streets = Most_Ticketed_Streets.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the geo locations for both datasets, let us learn more about how data is distributed in other columns. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to find out what is the status of tickets that are issued. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Total_Parking_Tickets['Status'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Status_count  = Total_Parking_Tickets['Status'].value_counts()\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(Status_count.index, Status_count.values, alpha=0.8)\n",
    "plt.title('Status of Parking Tickets')\n",
    "plt.ylabel('Number of Occurrences', fontsize=12)\n",
    "plt.xlabel('Status', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that around 10% of all tickets that are issued are eventually voided for some reason. What about the reasons these tickets are issued?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Total_Parking_Tickets['Bylaw'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Bylaw_count  = Total_Parking_Tickets['Bylaw'].value_counts()\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(Bylaw_count.index, Bylaw_count.values, alpha=0.8)\n",
    "plt.title('Bylaws Violated as per City of Vancouver')\n",
    "plt.ylabel('Number of Occurrences', fontsize=12)\n",
    "plt.xlabel('Bylaw', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The majority of tickets are issued for violationg Bylaws 2849 and 2952."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which parts of the city are the parking meters clustered in?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parking_meters['Geo Local Area'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Local_Area_count  = parking_meters['Geo Local Area'].value_counts()\n",
    "plt.figure(figsize=(10,5))\n",
    "chart =sns.barplot(Local_Area_count.index, Local_Area_count.values, alpha=0.8)\n",
    "plt.title('Bylaws Violated as per City of Vancouver')\n",
    "plt.ylabel('Number of Occurrences', fontsize=12)\n",
    "plt.xlabel('Bylaw', fontsize=12)\n",
    "chart.set_xticklabels(chart.get_xticklabels(), rotation=45, horizontalalignment='right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The meters are clustered in downtown mostly, with fairview coming a distant second."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us see how the tickets are distributed over the years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Yearly_dist = Total_Parking_Tickets.groupby('Year')['Bylaw'].value_counts().sort_index()\n",
    "Yearly_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x = Yearly_dist.index.get_level_values('Year'), y = Yearly_dist).set(\n",
    "    xlabel='Year', \n",
    "    ylabel='Number of Tickets'\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gradual increase in the number of tickets issued shows us the increase in number of cars plying on roads in Vancouver, leading to more competition to find that 'valued' parkign spot. Year 2020 is an exception. Covid-19 had an impact on the parking industry as well, and this project was done in September and hence contains data from only the first 8 months of the year."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about the monthly distribution? Are there some months were tickets tend to be issued more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Total_Parking_Tickets['Month'] = pd.DatetimeIndex(Total_Parking_Tickets['EntryDate']).month\n",
    "Total_Parking_Tickets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Monthly_dist = Total_Parking_Tickets.groupby('Month')['Bylaw'].value_counts().sort_index()\n",
    "Monthly_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x = Monthly_dist.index.get_level_values('Month'), y = Monthly_dist).set(\n",
    "    xlabel='Year', \n",
    "    ylabel='Number of Tickets'\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, the holiday season, at the end of the year and the month of January does lead to a bumper rise in revenue for the City of Vancouver through parking tickets. This might be due to increase to more cars being parked in more regions of the city. It would be great to dwelve on the reasons in coming months. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are the meters also located in the same places tickets are clustered? We will be using the foilum library to plot maps in our project. I have learnt the basics of this library from https://www.analyticsvidhya.com/blog/2020/06/guide-geospatial-analysis-folium-python/ .Let us make it work!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by looking at the overview of the city."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vancouver_Map = folium.Map(location = [49.2820, -123.1171], zoom_start=13)\n",
    "Vancouver_Map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start by loooking at a heat map of the most ticketed regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from folium import plugins\n",
    "from folium.plugins import HeatMap\n",
    "\n",
    "\n",
    "Vancouver_Map =folium.Map(location = [49.2820, -123.1171], zoom_start=13)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Most_Ticketed_Streets['Latitude'] = Most_Ticketed_Streets['Latitude'].astype(float)\n",
    "Most_Ticketed_Streets['Longitude'] = Most_Ticketed_Streets['Longitude'].astype(float)\n",
    "\n",
    "\n",
    "heat_df = Most_Ticketed_Streets[['Latitude', 'Longitude']]\n",
    "heat_df = Most_Ticketed_Streets.dropna(axis=0, subset=['Latitude','Longitude'])\n",
    "\n",
    "\n",
    "heat_data = [[row['Latitude'],row['Longitude']] for index, row in heat_df.iterrows()]\n",
    "\n",
    "\n",
    "HeatMap(heat_data).add_to(Vancouver_Map)\n",
    "\n",
    "Vancouver_Map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This library is working well. But we will need to see the exact locations to provide a precise solution. Let us see the exact locations of our parking meters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vancouver_Map =folium.Map(location = [49.2820, -123.1171], zoom_start=13)\n",
    "\n",
    "occurences = folium.map.FeatureGroup()\n",
    "\n",
    "\n",
    "for lat, lng in zip(parking_meters['Latitude'],\n",
    "                                         parking_meters['Longitude']):\n",
    "                                         \n",
    "                                        \n",
    "    occurences.add_child(\n",
    "        folium.vector_layers.CircleMarker(\n",
    "            [lat, lng],\n",
    "            radius=5, \n",
    "            color='green',\n",
    "            fill=True,\n",
    "            fill_color='purple',\n",
    "            fill_opacity=0.6,\n",
    "            \n",
    "        )\n",
    "    )\n",
    "\n",
    "Vancouver_Map.add_child(occurences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do the same for the top 5000 locations in our  Most_Ticketed_Streets dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vancouver_Map =folium.Map(location = [49.2820, -123.1171], zoom_start=13)\n",
    "\n",
    "occurences = folium.map.FeatureGroup()\n",
    "\n",
    "\n",
    "for lat, lng in zip(Most_Ticketed_Streets['Latitude'],\n",
    "                                         Most_Ticketed_Streets['Longitude']):\n",
    "                                         \n",
    "                                        \n",
    "    occurences.add_child(\n",
    "        folium.vector_layers.CircleMarker(\n",
    "            [lat, lng],\n",
    "            radius=5, \n",
    "            color='black',\n",
    "            fill=True,\n",
    "            fill_color='blue',\n",
    "            fill_opacity=0.6,\n",
    "            \n",
    "        )\n",
    "    )\n",
    "\n",
    "Vancouver_Map.add_child(occurences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that both of them are clustered in the same regions.This is a great win for our analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will be now using these maps to find out what other important differences exist in our meters that we would ultimately recommed to our drivers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start by analyzing which meters accept cash and which accept credit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parking_meters['CREDITCARD'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the majority of them accept cash. Let us map this out. We will use one-hot encoding to make a separate column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_card = parking_meters[['CREDITCARD','Longitude','Latitude']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot1 = pd.get_dummies(credit_card['CREDITCARD'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_card= pd.concat([credit_card, one_hot1], axis=1)\n",
    "credit_card"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use the colours in such a way that meters which accept cash or credit have different colours. The library we imported for this was import matplotlib.colors as colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_clusters_credit_card = folium.Map(location=[49.2820, -123.1171], zoom_start=4)\n",
    "\n",
    "# set color scheme for the clusters\n",
    "x = np.arange(2)\n",
    "ys = [i + x + (i*x)**2 for i in range(2)]\n",
    "colors_array = cm.rainbow(np.linspace(0, 1, len(ys)))\n",
    "rainbow = [colors.rgb2hex(i) for i in colors_array]\n",
    "\n",
    "# add markers to the map\n",
    "markers_colors = []\n",
    "for lat, lng, count in zip(credit_card['Latitude'], credit_card['Longitude'],  \n",
    "                                            credit_card['No']):\n",
    "                                        \n",
    "    label = folium.Popup(str(count) + '- Cluster', parse_html=True)\n",
    "    folium.vector_layers.CircleMarker(\n",
    "        [lat, lng],\n",
    "        radius=5,\n",
    "        popup=label,\n",
    "        tooltip = str(count) + '- Cluster',\n",
    "        color=rainbow[count-1],\n",
    "        fill=True,\n",
    "        fill_color=rainbow[count-2],\n",
    "        fill_opacity=0.1).add_to(map_clusters_credit_card)\n",
    "       \n",
    "map_clusters_credit_card"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeating the process to show the different types of meter heads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parking_meters['METERHEAD'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see here, there are more than 2 types of parking meters, hence we will use an encoding method instead of one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meter_head = parking_meters[['METERHEAD', 'Latitude', 'Longitude']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meter_head[\"METERHEAD\"]=parking_meters[\"METERHEAD\"].astype('category')\n",
    "meter_head[\"METERHEAD\"] = meter_head[\"METERHEAD\"].cat.codes\n",
    "meter_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_clusters_meterheads = folium.Map(location=[49.2820, -123.1171], zoom_start=4)\n",
    "# set color scheme for the clusters\n",
    "x = np.arange(6)\n",
    "ys = [i + x + (i*x)**2 for i in range(6)]\n",
    "colors_array = cm.rainbow(np.linspace(0, 1, len(ys)))\n",
    "rainbow = [colors.rgb2hex(i) for i in colors_array]\n",
    "\n",
    "# add markers to the map\n",
    "markers_colors = []\n",
    "for lat, lng, cluster in zip(meter_head['Latitude'], meter_head['Longitude'],  \n",
    "                                            meter_head['METERHEAD']):\n",
    "                                         \n",
    "    label = folium.Popup(str(cluster) + '- Cluster',parse_html=True)\n",
    "    folium.vector_layers.CircleMarker(\n",
    "        [lat, lng],\n",
    "        radius=5,\n",
    "        popup=label,\n",
    "        tooltip = str(cluster) + '- Cluster ',\n",
    "        color=rainbow[cluster-1],\n",
    "        fill=True,\n",
    "        fill_color=rainbow[cluster-1],\n",
    "        fill_opacity=0.9).add_to(map_clusters_meterheads )\n",
    "\n",
    "\n",
    "map_clusters_meterheads \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering Techniques\n",
    "\n",
    "Now that we know how our datasets are distributed, let us use clustering methods to identify specific clusters in our datasets. I learnt the concept I have applied here from a paid course that you can find here: https://www.coursera.org/learn/clustering-geolocation-data-intelligently-python/home/welcome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start by making a new datframe which includes our specific locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_weighted =Most_Ticketed_Streets.loc[:,['Address','Latitude','Longitude']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.geometry import Point, shape\n",
    "\n",
    "locs_geometry = [Point(xy) for xy in zip(X_weighted.Longitude,\n",
    "                                         X_weighted.Latitude)]\n",
    "crs = {'init': 'epsg:4326'}\n",
    "# Coordinate Reference Systems, \"epsg:4326\" is a common projection of WGS84 Latitude/Longitude\n",
    "locs_gdf = gpd.GeoDataFrame(X_weighted, crs=crs, geometry=locs_geometry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-Means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-Means is the most simple of all our clustering methods. Here, you do have to specify the number of clusters. (I used K value=4). K-Means Clustering then finds 4 number of clusters, given a centroid. First it assigns a random point as centroid and then make a cluster. Then it iteratively finds a better centroid and repeats the process until it settles on the given dataset. In this case there are 3 main clusters and 1 outlier. K-Means has the advantage of being more simple than other clustering techniques. However, you need to have the intuition about how many clusters exist in your data, which is difficult especially when dealing with higher dimensional data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "from ipywidgets import interactive\n",
    "from collections import defaultdict\n",
    "Z = np.array(X_weighted[['Longitude', 'Latitude']], dtype= 'float64')\n",
    "k=4\n",
    "model= KMeans(n_clusters=k, random_state=17).fit(Z)\n",
    "class_predictions=model.predict(Z)\n",
    "X_weighted[f'Cluster_kmeans_new{k}']= class_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are dealing with geo-locational data, we have to be really careful about interpreting the disatnces as pure numbers. You can read more about it here: https://www.thoughtco.com/degree-of-latitude-and-longitude-distance-4070616. Hence I have not used the most commonly used Inertia score here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy metric i have used here is Silhouette Score. It assigns a value of 1 or -1 to each point depending on the point;s closeness to the cluster it was assigned or to some other cluster. The closer the Silhouette Score is to 1, the better the accuracy. This is a much simpler method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'K={k}')\n",
    "print(f'Silhouette Score: {silhouette_score(Z, class_predictions)}')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_weighted['Cluster_kmeans_new4'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us map this out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "locs_map = folium.Map(location=[49.2827 , -123.1207],\n",
    "                      zoom_start=9, tiles='cartodbpositron')\n",
    "\n",
    "feature_ea = folium.FeatureGroup(name='Medium Risk')\n",
    "feature_pr = folium.FeatureGroup(name='Low Risk')\n",
    "feature_sr = folium.FeatureGroup(name='High Risk')\n",
    "\n",
    "for i, v in X_weighted.iterrows():\n",
    "    \n",
    "    \n",
    "    if v['Cluster_kmeans_new4'] == 1:\n",
    "        folium.CircleMarker(location=[v['Latitude'], v['Longitude']],\n",
    "                            radius=1,\n",
    "                            \n",
    "                            color='#FFBA00',\n",
    "                            fill_color='#FFBA00',\n",
    "                            fill=True).add_to(feature_ea)\n",
    "    elif v['Cluster_kmeans_new4'] == 0:\n",
    "        folium.CircleMarker(location=[v['Latitude'], v['Longitude']],\n",
    "                            radius=1,\n",
    "                    \n",
    "                            color='#087FBF',\n",
    "                            fill_color='#087FBF',\n",
    "                            fill=True).add_to(feature_pr)\n",
    "    elif v['Cluster_kmeans_new4'] == 3:\n",
    "        folium.CircleMarker(location=[v['Latitude'], v['Longitude']],\n",
    "                            radius=1,\n",
    "                           \n",
    "                            color='#FF0700',\n",
    "                            fill_color='#FF0700',\n",
    "                            fill=True).add_to(feature_sr)\n",
    "\n",
    "feature_ea.add_to(locs_map)\n",
    "feature_pr.add_to(locs_map)\n",
    "feature_sr.add_to(locs_map)\n",
    "folium.LayerControl(collapsed=False).add_to(locs_map)\n",
    "locs_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us find out the best k value where the silhouette score is the highest, for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from sklearn.metrics import silhouette_score\n",
    "best_silhouette, best_k = -1, 0\n",
    "for k in tqdm(range(2,10)):\n",
    "    model= KMeans(n_clusters=k, random_state=1).fit(Z)\n",
    "    class_predictions = model.predict(Z)\n",
    "    \n",
    "    curr_silhouette = silhouette_score(Z, class_predictions)\n",
    "    if curr_silhouette > best_silhouette:\n",
    "            best_k = k\n",
    "            best_silhouette = curr_silhouette\n",
    "print(f'K={k}')\n",
    "print(f'Silhouette Score: {best_silhouette}')   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can notice here, it is true that the higher the number of clusters, then better the Silhouette Score because it get easier for our model to assign a point to a smaller cluster. However, for the ease of undertsanding and classification we will use only 3 clusters- basically labellling each location as High, Medium or Low Risk.\n",
    "Later, we can try out more number of clusters for a more precise answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DBSCAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DBSCAN (Density-Based Spatial Clustering of Applications with Noise) is a more complicated yet precise model since it will look at the density of our locations. It considers a global density for all clusters which means it assumes that all clusters have the same density. Here you do not have to specify number of clusters. Using an epsilon value of 0.01 and min_samples value of 5(hyperparameters). Epsilon value is the maximum distance between 2 points of the same cluster. Minimum samples refers to the number of points surrounding a particular point for that point to be considered a core point. I was able to get a model that projected 3 clusters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "model= DBSCAN(eps=0.01, min_samples=5).fit(Z)\n",
    "class_predictions = model.labels_\n",
    "X_weighted[\"Clusters_DBSCAN\"] = class_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, here you have a chance to have more outliers. This is inbuilt in this model and do lead to a decrease in your accuracy as can be seen below. On this occassion, this model has very few outliers, which is a great sign. This means almost all of our locations fall in some cluster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of Clusters found: {len(np.unique(class_predictions))}')\n",
    "print(f'Number of outliers found: {len(class_predictions[class_predictions==-1])}')\n",
    "print(f'Silhouette ignoring outliers: {silhouette_score(Z[class_predictions!=-1], class_predictions[class_predictions!=-1])}')\n",
    "no_outliers = np.array([(counter+2)*x if x==-1 else x for counter, x in enumerate(class_predictions)])\n",
    "print(f'Silhouette outliers as singletons: {silhouette_score(Z, no_outliers)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Let us see the concentration of these clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_weighted['Clusters_DBSCAN'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "locs_map = folium.Map(location=[49.2827 , -123.1207],\n",
    "                      zoom_start=9, tiles='cartodbpositron')\n",
    "\n",
    "feature_ea = folium.FeatureGroup(name='Cluster 1 ')\n",
    "feature_pr = folium.FeatureGroup(name='Cluster 2 ')\n",
    "feature_sr = folium.FeatureGroup(name='Cluster 3 ')\n",
    "\n",
    "for i, v in X_weighted.iterrows():\n",
    "    \n",
    "    \n",
    "    if v['Clusters_DBSCAN'] == 0:\n",
    "        folium.CircleMarker(location=[v['Latitude'], v['Longitude']],\n",
    "                            radius=1,\n",
    "                            \n",
    "                            color='#FFBA00',\n",
    "                            fill_color='#FFBA00',\n",
    "                            fill=True).add_to(feature_ea)\n",
    "    elif v['Clusters_DBSCAN'] == 1:\n",
    "        folium.CircleMarker(location=[v['Latitude'], v['Longitude']],\n",
    "                            radius=1,\n",
    "                    \n",
    "                            color='#087FBF',\n",
    "                            fill_color='#087FBF',\n",
    "                            fill=True).add_to(feature_pr)\n",
    "    elif v['Clusters_DBSCAN'] == -1:\n",
    "        folium.CircleMarker(location=[v['Latitude'], v['Longitude']],\n",
    "                            radius=1,\n",
    "                           \n",
    "                            color='#FF0700',\n",
    "                            fill_color='#FF0700',\n",
    "                            fill=True).add_to(feature_sr)\n",
    "\n",
    "feature_ea.add_to(locs_map)\n",
    "feature_pr.add_to(locs_map)\n",
    "feature_sr.add_to(locs_map)\n",
    "folium.LayerControl(collapsed=False).add_to(locs_map)\n",
    "locs_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the model has not been able to distinguish the locations as well. If we ignore the outliers, the model does better than K-Means but falls behind if we take the outliers into consideration. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HDBSCAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are dealing with data that is denser in downtown and very parse in other regions, let us use a method that gives more weightage to density. HDBSCAN (Hierarchical DBSCAN) is a step ahead of the DBSCAN model. It should give us the best accuracy since we know that the tickets are denser in downtown and as we move to the outskirts of the city there are fewer tickets issued. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hdbscan\n",
    "model = hdbscan.HDBSCAN(min_cluster_size=5, min_samples=2, cluster_selection_epsilon=0.01)\n",
    "class_predictions = model.fit_predict(Z)\n",
    "X_weighted[\"Clusters_HDBSCAN\"] = class_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here again, I have used  similar hyperparameters to get same number of clusters. Remember, we can always change them to get different results as per our specific needs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of clusters found: {len(np.unique(class_predictions))}')\n",
    "print(f'Number of outliers found: {len(class_predictions[class_predictions==-1])}')\n",
    "print(f'Silhouette ignoring outliers: {silhouette_score(Z[class_predictions!=-1], class_predictions[class_predictions!=-1])}')\n",
    "no_outliers = np.array([(counter+2)*x if x==-1 else x for counter, x in enumerate(class_predictions)])\n",
    "print(f'Silhouette outliers as singletons: {silhouette_score(Z, no_outliers)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very similar results to DBSCAN, but better acccuracy, especially if you consider outliers! Let us plot the distribution here as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_weighted['Clusters_HDBSCAN'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "locs_map = folium.Map(location=[49.2827 , -123.1207],\n",
    "                      zoom_start=9, tiles='cartodbpositron')\n",
    "\n",
    "feature_ea = folium.FeatureGroup(name='Cluster 1 ')\n",
    "feature_pr = folium.FeatureGroup(name='Cluster 2 ')\n",
    "feature_sr = folium.FeatureGroup(name='Cluster 3 ')\n",
    "\n",
    "for i, v in X_weighted.iterrows():\n",
    "    \n",
    "    \n",
    "    if v['Clusters_HDBSCAN'] == 0:\n",
    "        folium.CircleMarker(location=[v['Latitude'], v['Longitude']],\n",
    "                            radius=1,\n",
    "                            \n",
    "                            color='#FFBA00',\n",
    "                            fill_color='#FFBA00',\n",
    "                            fill=True).add_to(feature_ea)\n",
    "    elif v['Clusters_HDBSCAN'] == 1:\n",
    "        folium.CircleMarker(location=[v['Latitude'], v['Longitude']],\n",
    "                            radius=1,\n",
    "                    \n",
    "                            color='#087FBF',\n",
    "                            fill_color='#087FBF',\n",
    "                            fill=True).add_to(feature_pr)\n",
    "    elif v['Clusters_HDBSCAN'] == -1:\n",
    "        folium.CircleMarker(location=[v['Latitude'], v['Longitude']],\n",
    "                            radius=1,\n",
    "                           \n",
    "                            color='#FF0700',\n",
    "                            fill_color='#FF0700',\n",
    "                            fill=True).add_to(feature_sr)\n",
    "\n",
    "feature_ea.add_to(locs_map)\n",
    "feature_pr.add_to(locs_map)\n",
    "feature_sr.add_to(locs_map)\n",
    "folium.LayerControl(collapsed=False).add_to(locs_map)\n",
    "locs_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, you can see a larger number of locations are included in the main cluster. We can change the hyperparameters to get more number of clusters that are more precise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Addressing Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we do provide parking recommendations to customers, we need to be sure about the risk of every location. Hence outliers will have to be grouped in some cluster. A disadvantage with DBCSAN and HDBCAN is they pick out outliers. For or purposes, we want to address the outlier problem and ensure all points are clustered in some way. For this we will use a Hybrid approach, combining our best model (DBSCAN) with K-Nearest Neighbors to assign each outlier the cluster than belongs to its nearest point. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "classifier= KNeighborsClassifier(n_neighbors=1)\n",
    "df_train= X_weighted[X_weighted.Clusters_HDBSCAN!=-1]\n",
    "df_predict= X_weighted[X_weighted.Clusters_HDBSCAN==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(df_train[['Longitude', 'Latitude']], dtype='float64')\n",
    "y_train = np.array(df_train['Clusters_HDBSCAN'])\n",
    "X_predict = np.array(df_predict[['Longitude', 'Latitude']], dtype='float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_weighted['Cluster_Hybrid'] = X_weighted['Clusters_HDBSCAN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_predictions = X_weighted.Cluster_Hybrid\n",
    "print(f'Number of Clusters found: {len(np.unique(class_predictions))}')\n",
    "print(f'Silhouette Score: {silhouette_score(Z, class_predictions)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, this might decrease the accuracy a bit, but it does serve our purpose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the difference between K-Means & Hybrid model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_weighted['Cluster_Hybrid'].value_counts().plot.hist(bins=3, alpha=0.4, label='Hybrid')\n",
    "X_weighted['Cluster_kmeans_new4'].value_counts().plot.hist(bins=3, alpha=0.4, label='Cluster_kmeans(3)')\n",
    "plt.legend()\n",
    "plt.title('Comparing Hybrid & K Means Approaches')\n",
    "plt.xlabel('Cluster_sizes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though all our models had similar accuracies, you can see here that using the hybrid model, there is a huge difference between the clusters. With K-Means, clusters are a bit intertwined. \n",
    "In the end, the ultimate decision to use a particular model depends on ease of undersatnding and use. In this case, K-Means is the preferred option which helps explain our project better. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Providing a customised solution to our customer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With all that knowledge, we can help provide a customised solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine someone is going to 1000 Cordova street from 1000 Hamilton Street. \n",
    "We need to plot 3 things here. \n",
    "\n",
    "1) The Route\n",
    "\n",
    "2) The Risk of Being Ticketed\n",
    "\n",
    "3)The location of specific parking meters which meet their requirements. \n",
    "\n",
    "Here the risk of being ticketed is an informed guess, knowing the concentration of parking tickets being issued."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "locs_map = folium.Map(location=[49.2827 , -123.1207],\n",
    "                      zoom_start=9, tiles='cartodbpositron')\n",
    "points_z = [[49.276537,-123.118980], [49.287335,-123.115269]]\n",
    "\n",
    "feature_ea = folium.FeatureGroup(name='Medium Risk')\n",
    "feature_pr = folium.FeatureGroup(name='Low Risk')\n",
    "feature_sr = folium.FeatureGroup(name='High Risk')\n",
    "\n",
    "for i, v in X_weighted.iterrows():\n",
    "    \n",
    "    \n",
    "    if v['Cluster_kmeans_new4'] == 1:\n",
    "        folium.CircleMarker(location=[v['Latitude'], v['Longitude']],\n",
    "                            radius=1,\n",
    "                            \n",
    "                            color='#FFBA00',\n",
    "                            fill_color='#FFBA00',\n",
    "                            fill=True).add_to(feature_ea)\n",
    "    elif v['Cluster_kmeans_new4'] == 0:\n",
    "        folium.CircleMarker(location=[v['Latitude'], v['Longitude']],\n",
    "                            radius=1,\n",
    "                    \n",
    "                            color='#087FBF',\n",
    "                            fill_color='#087FBF',\n",
    "                            fill=True).add_to(feature_pr)\n",
    "    elif v['Cluster_kmeans_new4'] == 3:\n",
    "        folium.CircleMarker(location=[v['Latitude'], v['Longitude']],\n",
    "                            radius=1,\n",
    "                           \n",
    "                            color='#FF0700',\n",
    "                            fill_color='#FF0700',\n",
    "                            fill=True).add_to(feature_sr)\n",
    "\n",
    "feature_ea.add_to(locs_map)\n",
    "feature_pr.add_to(locs_map)\n",
    "feature_sr.add_to(locs_map)\n",
    "folium.LayerControl(collapsed=False).add_to(locs_map)\n",
    "\n",
    "parking_points = []\n",
    "for lat, lng, in zip(parking_meters['Latitude'],\n",
    "                                         parking_meters['Longitude']):\n",
    "                                        \n",
    "\n",
    "        \n",
    "         \n",
    "                locs_map.add_child(\n",
    "                folium.Marker([lat, lng], popup='Timberline Lodge',\n",
    "                               icon=folium.Icon(icon=\"credit-card\", prefix=\"fa\")\n",
    "             )\n",
    "    )                               \n",
    "# add markers 'z'\n",
    "for each in points_z:  \n",
    "    locs_map.add_child(folium.CircleMarker(location=each,\n",
    "    fill='true',\n",
    "    radius = 6,\n",
    "    popup= 'Bye',\n",
    "    fill_color='yellow',\n",
    "    color = 'clear',\n",
    "    fill_opacity=1))\n",
    "    \n",
    "for each in points_z:  \n",
    "    folium.Marker(each).add_to(locs_map)\n",
    "\n",
    "folium.PolyLine(points_z, color=\"black\", weight=2.5, opacity=1).add_to(locs_map)\n",
    "\n",
    "\n",
    "locs_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the driver has multiple options to choose from and can go to the parking spot nearest to them which is available. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project gives us endless possibilities of additions/customizations to pour earlier findings. We can specify the time and select only those parking meters that are open during that time. We can choose the cheapest option as well. Another addition would be the time of the year, day which will make it even more realistic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following these footsteps, we can create new examples as well. I can team up with a web developer to make a full-fledged application that allows users to make real time decisions. We can also include the weather updates, traffic movements and even off-street parking options in our analysis.\n",
    "There is huge opportunity in this space to make further updates and use this information for the benefit of the general public. I thank the Education Team and my peers who provided much needed support for this project.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
